{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237ca1c5-90b9-4415-a326-c04b1ec3eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import mido\n",
    "from pathlib import Path\n",
    "import os\n",
    "from miditok import MIDILike, TokenizerConfig, REMI\n",
    "from miditok.pytorch_data import DataCollator\n",
    "import math\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm import tqdm\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14fc11e-7b60-4ce2-a2be-b142084d82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 478\n",
    "    n_layers: int = 6\n",
    "    n_heads: int = 8\n",
    "    n_embed: int = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f7ecf-d664-4f42-bac6-0029f3f2e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_CONFIG = TokenizerConfig(\n",
    "    num_velocities=32,\n",
    "    use_programs=False,\n",
    "    use_time_signatures=False,\n",
    "    use_chords=False,\n",
    "    use_sustain_pedals=True,\n",
    ")\n",
    "\n",
    "remi_config = TokenizerConfig(\n",
    "    use_programs=False,\n",
    "    num_velocities=32,\n",
    "    use_sustain_pedals=True,\n",
    "    use_time_signatures=False,\n",
    "    use_chords=False,\n",
    "    use_tempos=True,\n",
    "    use_rests=True,\n",
    "    beat_res={(0, 4): 8, (4, 12): 16},\n",
    "    use_bars=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc40ae54-8f3e-45f8-8bfa-107341b2db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = REMI(tokenizer_config=remi_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26b7635-030a-41f1-a490-a20eac03fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIDI_DIR = Path(\"maestro-v3.0.0\")\n",
    "TOKEN_DIR = Path(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c7d17d-8c1f-4e56-a436-530d32b67004",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_paths = list(MIDI_DIR.glob(\"**/*.mid\")) + list(MIDI_DIR.glob(\"**/*.midi\"))\n",
    "if midi_paths and not any(TOKEN_DIR.iterdir()):\n",
    "    tokenizer.tokenize_dataset(midi_paths, TOKEN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4c89bf-ad13-4b08-9c34-3bb310bd181f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9443a4-7871-4b90-a2ba-9a857c0a2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, tokens_dir: Path, block_size: int, tokenizer, stride=None, augment=True):\n",
    "        self.block_size = block_size\n",
    "        self.stride = stride if stride is not None else block_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.augment = augment\n",
    "\n",
    "        # special tokens\n",
    "        self.pad_id = tokenizer.special_tokens_ids[0]  # PAD_None\n",
    "        self.bos_id = tokenizer.special_tokens_ids[1]  # BOS_None\n",
    "        self.eos_id = tokenizer.special_tokens_ids[2]  # EOS_None\n",
    "        self.mask_id = tokenizer.special_tokens_ids[3]\n",
    "\n",
    "        self.samples = []\n",
    "\n",
    "        for path in tokens_dir.glob(\"**/*.json\"):\n",
    "            list_of_tracks = tokenizer.load_tokens(path)\n",
    "            for ids in list_of_tracks:\n",
    "                ids = [self.bos_id] + list(ids) + [self.eos_id]\n",
    "\n",
    "                for i in range(0, len(ids), self.stride):\n",
    "                    chunk = ids[i : i + block_size + 1]\n",
    "                    if len(chunk) < block_size + 1:\n",
    "                        pad_chunk = torch.full((block_size + 1,), self.pad_id, dtype=torch.long)\n",
    "                        pad_chunk[: len(chunk)] = torch.tensor(chunk, dtype=torch.long)\n",
    "                        chunk = pad_chunk\n",
    "                    else:\n",
    "                        chunk = torch.tensor(chunk, dtype=torch.long)\n",
    "                    self.samples.append(chunk)\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} chunks (stride={self.stride}) from {tokens_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.samples[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            chunk = self.apply_augmentation(chunk.clone())\n",
    "\n",
    "        return chunk[:-1], chunk[1:]\n",
    "\n",
    "    def apply_augmentation(self, chunk: torch.Tensor) -> torch.Tensor:\n",
    "        tokens = [self.tokenizer[t] for t in chunk.tolist()]\n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            transpose = np.random.randint(-6, 7)\n",
    "            tokens = self.transpose(tokens, transpose)\n",
    "\n",
    "        if np.random.rand() < 0.3:\n",
    "            jitter = np.random.randint(-2, 3)\n",
    "            tokens = self.jitter_velocity(tokens, jitter)\n",
    "\n",
    "        if np.random.rand() < 0.3:\n",
    "            tokens = self.jitter_timing(tokens)\n",
    "\n",
    "        return torch.tensor([self.tokenizer.vocab_model.get(t, self.tokenizer.special_tokens_ids[0]) for t in tokens], dtype=torch.long)\n",
    "\n",
    "    def transpose(self, tokens, semitones):\n",
    "        new_tokens = []\n",
    "        for t in tokens:\n",
    "            if t.startswith(\"Note-On\") or t.startswith(\"Note-Off\"):\n",
    "                note_type, pitch = t.split(\"_\")\n",
    "                pitch = int(pitch)\n",
    "                pitch = max(0, min(127, pitch + semitones))\n",
    "                new_tokens.append(f\"{note_type}_{pitch}\")\n",
    "            else:\n",
    "                new_tokens.append(t)\n",
    "        return new_tokens\n",
    "\n",
    "    def jitter_velocity(self, tokens, jitter):\n",
    "        new_tokens = []\n",
    "        nb_velocities = self.tokenizer.config.num_velocities\n",
    "        for t in tokens:\n",
    "            if t.startswith(\"Velocity\"):\n",
    "                _, vel = t.split(\"_\")\n",
    "                vel = int(vel)\n",
    "                vel = max(0, min(nb_velocities-1, vel + jitter))\n",
    "                new_tokens.append(f\"Velocity_{vel}\")\n",
    "            else:\n",
    "                new_tokens.append(t)\n",
    "        return new_tokens\n",
    "\n",
    "    def jitter_timing(self, tokens):\n",
    "        new_tokens = []\n",
    "        for t in tokens:\n",
    "            if t.startswith(\"Time-Shift\"):\n",
    "                _, shift = t.split(\"_\")\n",
    "                shift = int(shift)\n",
    "                shift = max(1, shift + np.random.choice([-1, 0, 1]))\n",
    "                new_tokens.append(f\"Time-Shift_{shift}\")\n",
    "            else:\n",
    "                new_tokens.append(t)\n",
    "        return new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089f7c8-3241-48fd-b537-db10938f808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, head_dim: int, max_len: int):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        theta = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        m = torch.arange(max_len)\n",
    "        freqs = torch.outer(m, theta)\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "        \n",
    "        self.register_buffer(\"freqs_cis\", freqs_cis, persistent=False)\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        q_ = q.float().reshape(*q.shape[:-1], -1, 2)\n",
    "        k_ = k.float().reshape(*k.shape[:-1], -1, 2)\n",
    "        \n",
    "        q_complex = torch.view_as_complex(q_)\n",
    "        k_complex = torch.view_as_complex(k_)\n",
    "        \n",
    "        freqs_cis = self.freqs_cis[None, :q.shape[1], None, :]\n",
    "        \n",
    "        q_rotated = torch.view_as_real(q_complex * freqs_cis).flatten(3)\n",
    "        k_rotated = torch.view_as_real(k_complex * freqs_cis).flatten(3)\n",
    "        \n",
    "        return q_rotated.type_as(q), k_rotated.type_as(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e58473-960a-4780-b54b-7a2c7624b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig, rope: RoPE):\n",
    "        super().__init__()\n",
    "\n",
    "        self.qkv_proj = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.o_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.o_proj.SCALE = 1\n",
    "\n",
    "        self.d_model = config.n_embed\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "\n",
    "        self.rope = rope\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.split(self.d_model, dim=2)\n",
    "        q = q.view(B, T, self.n_heads, self.d_head).transpose(1,2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_head).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_head).transpose(1,2)\n",
    "\n",
    "        q, k = self.rope(q, k)\n",
    "\n",
    "        attn_score = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        attn_score = attn_score.transpose(1,2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.o_proj(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34f5c36-f773-483b-bc72-9ee7b03505f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff1 = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.ff2 = nn.Linear(config.n_embed*4, config.n_embed)\n",
    "        self.ff2.SCALE = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ff1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.ff2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2029c13-f863-4fb4-b52c-dc99821cfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig, rope: RoPE):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
    "        self.attn = SelfAttention(config, rope)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
    "        self.ff = MLP(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.ln1(x)))\n",
    "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ddd0d-fa10-453d-89f4-f213763cdb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.pad_id = tokenizer.special_tokens_ids[0]\n",
    "\n",
    "        head_dim = config.n_embed // config.n_heads\n",
    "        rope = RoPE(head_dim, config.block_size)\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed),\n",
    "            h = nn.ModuleList([Layer(config, rope) for _ in range(config.n_layers)]),\n",
    "            ln_final = nn.LayerNorm(config.n_embed)\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'SCALE'):\n",
    "                std *= (2 * self.config.n_layers) ** -0.5 \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "        \n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        B, T = x.size()\n",
    "        assert T <= self.config.block_size, \"cannot exceed block size\"\n",
    "\n",
    "        x = self.transformer.wte(x)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_final(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=self.pad_id, label_smoothing=0.1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9487c2-87d0-43f0-ae2f-30f5085ca7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLOCK_SIZE = TransformerConfig.block_size\n",
    "BLOCK_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697bd5e5-06c9-4fb7-bd71-2e4bb21382b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EFFECTIVE_BATCH_SIZE = 32\n",
    "accum_steps = EFFECTIVE_BATCH_SIZE // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ff4bf3-f1c2-46a8-a43f-51e87c228e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28549 chunks (stride=1024) from tokens\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = MidiDataset(TOKEN_DIR, BLOCK_SIZE, tokenizer, stride=BLOCK_SIZE//2, augment=False)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e726a060-9d1b-4da8-9f3e-8c2d26e8c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = torch.device(\"cuda\")\n",
    "model = MusicTransformer(config=TransformerConfig).to(device)\n",
    "model = torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.999), fused=True, weight_decay=0.01)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c405fa98-2334-4680-8f2c-4fb4e08543f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sequence(model, tokenizer, device, prompt_tokens, max_new_tokens, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([prompt_tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    for _ in tqdm(range(max_new_tokens), desc=\"Generating\"):\n",
    "        idx_cond = idx if idx.size(1) <= model.config.block_size else idx[:, -model.config.block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        if idx_next.item() == tokenizer['EOS_None']:\n",
    "            break\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    return idx[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ac048d-f7a8-4e91-a114-e11226236668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_midi(tokens, tokenizer, output_midi_path=\"output(1).mid\"):\n",
    "    score_object = tokenizer.decode([tokens])\n",
    "    output_path = Path(output_midi_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    score_object.dump_midi(str(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "149a2ad0-4573-4647-8980-873c8c38d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6dbf909-63fc-428e-b891-51735bc5684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ca207-4ec0-4e82-a1a4-e345e5150d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        \n",
    "        for i, (input_ids, labels) in enumerate(progress_bar):\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                _, loss = model(input_ids, targets=labels)\n",
    "                loss = loss / accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if not torch.isfinite(loss):\n",
    "                print(f\"[WARNING] Loss is nan/inf, skipping step\")\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                continue\n",
    "\n",
    "            if (i+1) % accum_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            total_loss += loss.item() * accum_steps\n",
    "            progress_bar.set_postfix(loss=loss.item() * accum_steps)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels) in tqdm(test_dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    _, loss = model(inputs, targets=labels)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        avg_val_loss = val_loss / len(test_dataloader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch) % 15 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "            }, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            prompt = [tokenizer['BOS_None']]\n",
    "            generated_tokens = generate_sequence(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                device,\n",
    "                prompt_tokens=prompt,\n",
    "                max_new_tokens=2048\n",
    "            )\n",
    "            tokens_to_midi(generated_tokens[1:], tokenizer, output_midi_path=f\"output_{epoch+1}.mid\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160317fc-511a-4d68-9579-4be017871daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sequence(model, tokenizer, device, prompt_tokens, max_new_tokens, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([prompt_tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    for _ in tqdm(range(max_new_tokens), desc=\"Generating\"):\n",
    "        idx_cond = idx if idx.size(1) <= model.config.block_size else idx[:, -model.config.block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        if idx_next.item() == tokenizer['EOS_None']:\n",
    "            break\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    return idx[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f4de5ef-a5eb-4481-ada9-b6e3964ddc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_midi(tokens, tokenizer, output_midi_path=\"output_test(1).mid\"):\n",
    "    score_object = tokenizer.decode([tokens])\n",
    "    output_path = Path(output_midi_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    score_object.dump_midi(str(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78004368-13fb-4b80-98aa-6e2bb4a16a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
